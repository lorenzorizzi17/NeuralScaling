{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1a3ec8",
   "metadata": {},
   "source": [
    "## Kernel limit for a Random Fourier Feature model\n",
    "\n",
    "The formula for the deterministic equivalent $R_{n,p}$ can be applied for every value of $p$. In addition, when $p \\to \\infty$, the random feature model reduces to a kernel machine where:\n",
    "$$\n",
    "K(x,x') = \\mathbb{E}_{w \\in \\mu(w)}[\\varphi(x, w) \\varphi(x', w)]\n",
    "$$\n",
    "As shown in [Rahimi and Recht](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf), if we let $\\phi(x,w) = \\cos(\\langle w,x \\rangle)$ and $w \\in \\mathcal{N}(0, \\gamma\\mathbf{1}_d)$, then the liming kernel is the usual RBF kernel $K(x,x') =\\exp(-\\gamma\\frac{||x-x'||^2}{2})$.\n",
    "\n",
    "In this notebook, we aim at verifying whether the predictions for the deterministic equivalent in the kernel limit $p \\to \\infty$ are in agreement with the risk induced by a RBF kernel ridge regression problem. In particular, we will proceed as shown in Appendix C.3 Empirical Diagonalization:\n",
    "1) We sample $N$ points $\\vec x \\in \\mathbb{R}^d$ from $\\mathcal{N}(0, \\mathbf{1}_d)$\n",
    "2) We sample $P$ points $\\vec w \\in \\mathbb{R}^p$ from $\\mathcal{N}(0, \\gamma \\mathbf{1}_d)$\n",
    "3) We can now approximate the PDF over $\\vec w$ and write $K(x,x') \\approx \\sum_j^P \\varphi(\\vec x,\\vec w_j) \\varphi(\\vec x',\\vec w_j)$ where $\\varphi(\\vec x, \\vec w) = \\frac{1}{\\sqrt{p}}\\cos(\\langle \\vec x, \\vec w \\rangle)$\n",
    "4) Starting from the $N$ points $\\vec x_i$, we'll build the $N\\times N$ matrix $\\tilde K$ where $\\tilde K_{ij} = \\frac{1}{N} K(\\vec x_i, \\vec x_j)$\n",
    "5) The eigenvalues $\\tilde \\xi_k^2$ approximate the true eigenvalues $\\xi_k^2$ of $\\Sigma$ for large enough $N$\n",
    "6) We obtain the decomposition of $f_\\star(\\vec x)$ with $\\beta_{\\star,k} \\approx \\tilde \\beta_k = \\frac{1}{N} \\vec y^T \\tilde \\psi_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8e993ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09165dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the empirical diagonalization of the kernel matrix...\n",
    "def empiricalDiagonalization(target_function, gamma = 0.5):\n",
    "    N = 5000   # as large as possible\n",
    "    P = 5000 \n",
    "    d = 1000\n",
    "\n",
    "    X = np.random.randn(N, d)\n",
    "    W = np.random.randn(d, P) * np.sqrt(2 * gamma)\n",
    "    b = np.random.rand(1, P) * 2 * np.pi\n",
    "\n",
    "    Projection = X @ W \n",
    "    Phi = np.sqrt(2.0 / P) * np.cos(Projection + b)\n",
    "    Ka = Phi @ Phi.T \n",
    "\n",
    "    vals, vecs = np.linalg.eigh(Ka)\n",
    "    vals = vals / N\n",
    "    vals = vals[::-1]\n",
    "    vecs = vecs[:, ::-1]\n",
    "\n",
    "    y = target_function(X) \n",
    "    betas = (1 / np.sqrt(N)) * (vecs.T @ y)\n",
    "    return np.array(vals), betas\n",
    "\n",
    "# Solver for nu_K in the kernel limit\n",
    "def solveNu1KernelLimit(n, lambd, Sigma, tolerance=1e-10, maxIter=1000):\n",
    "    nu = lambd \n",
    "    \n",
    "    for it in range(maxIter):\n",
    "        df = np.sum(Sigma / (Sigma + nu))\n",
    "        nu_new = lambd + (nu / n) * df\n",
    "        \n",
    "        if np.abs(nu_new - nu) < tolerance:\n",
    "            return nu_new\n",
    "        nu = nu_new\n",
    "        \n",
    "    return nu\n",
    "\n",
    "# Compute the deterministic equivalent of the variance and bias (same as in previous notebook, but specialized for kernel limit)\n",
    "def computeDeterministicEquivalent(n : int, lambd : float, noise : float, Sigma : np.array, beta : np.array):\n",
    "    nu1 = solveNu1KernelLimit(n, lambd, Sigma)\n",
    "\n",
    "    BiasNum = nu1**2 * np.sum(beta**2 / (Sigma + nu1)**2 )\n",
    "    BiasDen = 1 - (1/n) * np.sum(Sigma**2 / (Sigma+nu1)**2)\n",
    "    Bias = BiasNum / BiasDen\n",
    "\n",
    "    VarNum =noise**2 * np.sum(Sigma**2 / (Sigma+nu1)**2)\n",
    "    VarDen = n - np.sum(Sigma**2 / (Sigma+nu1)**2)\n",
    "    Var = VarNum/VarDen\n",
    "\n",
    "    return Var, Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce1094e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m lambd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Approximate eigenvalues xi_k and beta decomposition with the empirical diagonalization procedure\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m Sigma, beta \u001b[38;5;241m=\u001b[39m \u001b[43mempiricalDiagonalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m Var, Bias \u001b[38;5;241m=\u001b[39m computeDeterministicEquivalent(NSamples, lambd, noise, Sigma, beta)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeterministic Equivalent - Variance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVar\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Bias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBias\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVar\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mBias\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 20\u001b[0m, in \u001b[0;36mempiricalDiagonalization\u001b[0;34m(target_function, gamma)\u001b[0m\n\u001b[1;32m     17\u001b[0m vals \u001b[38;5;241m=\u001b[39m vals[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m vecs \u001b[38;5;241m=\u001b[39m vecs[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     21\u001b[0m betas \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(N)) \u001b[38;5;241m*\u001b[39m (vecs\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(vals), betas\n",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m, in \u001b[0;36mtarget_function\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      5\u001b[0m w_true \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(w_true)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Target = sin(3 * x_proj) + parte quadratica\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m proj \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_true\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m proj) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (proj\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 1000)"
     ]
    }
   ],
   "source": [
    "# Let us define our target function f\n",
    "def target_function(X):\n",
    "    # Usiamo una proiezione casuale fissa per definire il target\n",
    "    w_true = np.random.randn(100) \n",
    "    w_true /= np.linalg.norm(w_true)\n",
    "    # Target = sin(3 * x_proj) + parte quadratica\n",
    "    proj = X @ w_true\n",
    "    return np.sin(3 * proj) + 0.1 * (proj**2)\n",
    "\n",
    "# And now create our KRR problem ... Number of samples:\n",
    "NSamples = 600\n",
    "# and their dimension\n",
    "d = 100\n",
    "\n",
    "X = np.random.randn(NSamples, d)\n",
    "noise = 0.1\n",
    "y = target_function(X) + noise * np.random.randn(NSamples)\n",
    "# Regularization parameter and gamma parameter for RBF kernel\n",
    "gamma = 0.5\n",
    "lambd = 1e-5\n",
    "# Approximate eigenvalues xi_k and beta decomposition with the empirical diagonalization procedure\n",
    "Sigma, beta = empiricalDiagonalization(target_function=target_function, gamma = gamma)\n",
    "Var, Bias = computeDeterministicEquivalent(NSamples, lambd, noise, Sigma, beta)\n",
    "print(f\"Deterministic Equivalent - Variance: {Var:.6f}, Bias: {Bias:.6f}, Total: {Var + Bias:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90bda4",
   "metadata": {},
   "source": [
    "The computation of the deterministic equivalent is supposed to be perfectly deterministic; however, the empirical diagonalization procedure injects stochasticity into there results.\n",
    "\n",
    "Now given the same Kernel Ridge regression problem, let's compute the empirical risk using SciKitLearn KRR API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92037a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical risk for the KRR model: 0.539619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# Build and train the KRR model\n",
    "krr = KernelRidge(kernel='rbf', alpha=NSamples*lambd, gamma=0.5)\n",
    "krr.fit(X, y)\n",
    "# To approximate the true risk, we can use a large test set and approximate via Monte Carlo\n",
    "Ntest = 10000\n",
    "X_test = np.random.randn(10000, d)\n",
    "y_test = target_function(X_test)\n",
    "y_pred = krr.predict(X_test)    \n",
    "risk = np.mean((y_test - y_pred)**2)\n",
    "\n",
    "#test_risk = get_risk_sklearn(krr, X_test, y_test)\n",
    "\n",
    "print(f\"Empirical risk for the KRR model: {risk:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17157e",
   "metadata": {},
   "source": [
    "The results seems to be nicely in agreement, suggesting the validity of the kernel limit for $R_{n,p}$. Let's try and evalute the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64f41bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# --- 1. CALCOLO SPETTRO OPERATORE (UNA VOLTA SOLA) ---\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Sigma e beta dipendono dalla distribuzione dei dati e dalla funzione target,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# NON da quanti dati usi per il training. Calcolali su una popolazione grande.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Nota: Assicurati che la tua funzione empiricalDiagonalization usi un N interno grande (es. 2000 o 5000)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m Sigma_op, beta_op \u001b[38;5;241m=\u001b[39m \u001b[43mempiricalDiagonalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- 2. LOOP SUI SAMPLE SIZE ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m NValues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m4001\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 20\u001b[0m, in \u001b[0;36mempiricalDiagonalization\u001b[0;34m(target_function, gamma)\u001b[0m\n\u001b[1;32m     17\u001b[0m vals \u001b[38;5;241m=\u001b[39m vals[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m vecs \u001b[38;5;241m=\u001b[39m vecs[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     21\u001b[0m betas \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(N)) \u001b[38;5;241m*\u001b[39m (vecs\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(vals), betas\n",
      "Cell \u001b[0;32mIn[54], line 7\u001b[0m, in \u001b[0;36mtarget_function\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      5\u001b[0m w_true \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(w_true)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Target = sin(3 * x_proj) + parte quadratica\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m proj \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_true\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m proj) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (proj\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 1000)"
     ]
    }
   ],
   "source": [
    "# --- PARAMETRI FISSI ---\n",
    "d = 100\n",
    "gamma = 0.01      # 1/d è lo scaling corretto. 0.5 era troppo grande.\n",
    "lambd = 1e-5\n",
    "noise = 0.1\n",
    "\n",
    "# --- 1. CALCOLO SPETTRO OPERATORE (UNA VOLTA SOLA) ---\n",
    "# Sigma e beta dipendono dalla distribuzione dei dati e dalla funzione target,\n",
    "# NON da quanti dati usi per il training. Calcolali su una popolazione grande.\n",
    "# Nota: Assicurati che la tua funzione empiricalDiagonalization usi un N interno grande (es. 2000 o 5000)\n",
    "Sigma_op, beta_op = empiricalDiagonalization(target_function, gamma=gamma)\n",
    "\n",
    "# --- 2. LOOP SUI SAMPLE SIZE ---\n",
    "NValues = np.arange(50, 4001, 200)\n",
    "EmpiricalRisks = []\n",
    "DeterministicRisks = []\n",
    "\n",
    "for N in NValues:\n",
    "    NSamples = N\n",
    "\n",
    "    # Generazione Training Set corrente\n",
    "    X = np.random.randn(NSamples, d)\n",
    "    y = target_function(X) + noise * np.random.randn(NSamples)\n",
    "    \n",
    "    # --- TEORIA (Deterministic Equivalent) ---\n",
    "    # Usiamo Sigma_op e beta_op calcolati fuori\n",
    "    Var, Bias = computeDeterministicEquivalent(NSamples, lambd, noise, Sigma_op, beta_op)\n",
    "    DeterministicRisks.append(Var + Bias)\n",
    "\n",
    "    # --- SIMULAZIONE (Kernel Ridge Regression) ---\n",
    "    # IMPORTANTE: Usa la variabile 'gamma', non 0.5 hardcodato!\n",
    "    krr = KernelRidge(kernel='rbf', alpha=NSamples*lambd, gamma=gamma)\n",
    "    krr.fit(X, y)\n",
    "    \n",
    "    # Test (Monte Carlo approximation of Risk)\n",
    "    # Usiamo un test set fisso o rigenerato (qui rigenerato va bene)\n",
    "    X_test = np.random.randn(2000, d) # 2000 è sufficiente per una stima veloce\n",
    "    y_test = target_function(X_test)\n",
    "    y_pred = krr.predict(X_test)    \n",
    "    \n",
    "    risk = np.mean((y_test - y_pred)**2)\n",
    "    EmpiricalRisks.append(risk)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(NValues, EmpiricalRisks, label='Empirical Risk (KRR)', marker='o', alpha=0.6)\n",
    "plt.plot(NValues, DeterministicRisks, label='Deterministic Equivalent', marker='x', linestyle='--')\n",
    "plt.xlabel('Number of Samples (N)')\n",
    "plt.ylabel('Risk (MSE)')\n",
    "plt.title(f'Empirical vs Deterministic (d={d}, $\\gamma$={gamma})')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.7)\n",
    "plt.yscale('log') # La scala logaritmica aiuta a vedere meglio la discesa\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
